{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4185120c-a86b-4a02-84c7-8da04aa45956",
   "metadata": {},
   "source": [
    "The following notebook is based on: https://github.com/pashtari/factorizer-isles22/blob/master/get_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ad9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c286c2df-d770-40b1-aee7-4a6129e521e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6363045-5309-4cb0-b988-29f080f03a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/pashtari/factorizer.git@0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6ff387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import factorizer as ft\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import pytorch_lightning as pl\n",
    "from monai import transforms\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5848aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n"
     ]
    }
   ],
   "source": [
    "print(12*12*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91900275-988d-43d7-95fb-693e6eb64c87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d069829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin_factorizer = ft.factorizer(\n",
    "#     in_channels=4,\n",
    "#     out_channels=3,\n",
    "#     spatial_size=(8, 8, 8),\n",
    "#     encoder_depth=(1, 1, 1),\n",
    "#     encoder_width=(2, 2, 2),\n",
    "#     strides=(1, 2, 2),\n",
    "#     decoder_depth=(1, 1, 1, 1),\n",
    "#     norm=ft.LayerNorm,\n",
    "#     reshape=(ft.SWMatricize, {'head_dim': 2, 'patch_size': 2}),\n",
    "#     act=nn.ReLU,\n",
    "#     factorize=ft.NMF,\n",
    "#     rank=1,\n",
    "#     num_iters=5,\n",
    "#     init=\"uniform\",\n",
    "#     solver=\"hals\",\n",
    "#     mlp_ratio=2,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# x = torch.rand((1, 4, 8, 8, 8))\n",
    "\n",
    "# y = swin_factorizer(x)\n",
    "# print(\"Output shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f1644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(swin_factorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5db5-b9b2-4d1a-b9f6-fbce70d759ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check image shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d485fe2-e96d-4705-8f3d-e9ca044b423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/Data/\"))\n",
    "print(os.path.exists(\"/Data/data/isles/sub-strokecase0001/ses-0001/anat/sub-strokecase0001_ses-0001_flair_registered.nii.gz\"))\n",
    "print(os.path.exists(\"Data/data/isles/sub-strokecase0100/ses-0001/dwi/sub-strokecase0100_ses-0001_dwi.nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9123b900-cd6c-4ef4-b3b1-db74a24dc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set data path and\n",
    "# dataset_dir = \"/Data/data/isles\"\n",
    "\n",
    "# # set patient ID and images path\n",
    "# id_ = \"sub-strokecase0001\"\n",
    "# dwi_path = f\"{dataset_dir}/{id_}/ses-0001/dwi/{id_}_ses-0001_dwi.nii.gz\"\n",
    "# adc_path = f\"{dataset_dir}/{id_}/ses-0001/dwi/{id_}_ses-0001_adc.nii.gz\"\n",
    "\n",
    "# msk_path = f\"{dataset_dir}/derivatives/{id_}/ses-0001/{id_}_ses-0001_msk.nii.gz\"\n",
    "\n",
    "# # make data dictionary\n",
    "# data = {\n",
    "#     \"image\": [dwi_path, adc_path],\n",
    "#     \"mask\": msk_path,\n",
    "# }\n",
    "\n",
    "# load_image = transforms.LoadImaged(\n",
    "#     [\"image\", \"mask\"],\n",
    "#     ensure_channel_first=True,\n",
    "#     allow_missing_keys=True,\n",
    "# )\n",
    "\n",
    "# # load image data\n",
    "# data = load_image(data)\n",
    "# print(f\"image shape: {data['image'].shape}\")\n",
    "# print(f\"mask shape: {data['mask'].shape}\")\n",
    "\n",
    "\n",
    "# dwi_image = data[\"image\"][0]\n",
    "# adc_image = data[\"image\"][1]\n",
    "# msk_image = data[\"mask\"][0]\n",
    "\n",
    "# # pick a slice with the largest lesion volume for visualization\n",
    "# slc = msk_image.sum((0, 1)).argmax()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 3, dpi=200)\n",
    "# # visulize DWI image\n",
    "# ax[0].imshow(dwi_image[:, :, slc], cmap=\"gray\", origin=\"lower\")\n",
    "# ax[0].set_title(\"DWI\")\n",
    "# ax[0].set_axis_off()\n",
    "\n",
    "# # visulize ADC image\n",
    "# ax[1].imshow(adc_image[:, :, slc], cmap=\"gray\", origin=\"lower\")\n",
    "# ax[1].set_title(\"ADC\")\n",
    "# ax[1].set_axis_off()\n",
    "\n",
    "# # visulize mask\n",
    "# ax[2].imshow(dwi_image[:, :, slc], \"gray\", origin=\"lower\")\n",
    "# masked = np.ma.masked_where(msk_image[:, :, slc] == 0, dwi_image[:, :, slc])\n",
    "# ax[2].imshow(masked, ListedColormap([\"red\"]), alpha=0.9, origin=\"lower\")\n",
    "# ax[2].set_title(\"Ground Truth\")\n",
    "# ax[2].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7daa9-012a-4835-ab01-4315227df894",
   "metadata": {},
   "source": [
    "## Setup transforms for training and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f29b349a-bf33-497d-b6b3-cd1b6be5a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    train_transform = [\n",
    "        ft.ReadImaged([\"image\", \"label\"], ensure_channel_first=True),\n",
    "        transforms.SqueezeDimd(\"image\", dim=1),\n",
    "        transforms.CropForegroundd([\"image\", \"label\"], source_key=\"image\"),\n",
    "        transforms.NormalizeIntensityd(\"image\", nonzero=True, channel_wise=True),\n",
    "        transforms.Spacingd(\n",
    "            [\"image\", \"label\"],\n",
    "            pixdim=(2.0, 2.0, 2.0),\n",
    "            mode=(\"bilinear\", \"bilinear\"),\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            [\"image\", \"label\"], roi_size=(64, 64, 64), random_size=False\n",
    "        ),\n",
    "        transforms.RandAffined(\n",
    "            [\"image\", \"label\"],\n",
    "            prob=0.15,\n",
    "            spatial_size=(64, 64, 64),\n",
    "            rotate_range=[30 * np.pi / 180] * 3,\n",
    "            scale_range=[0.3] * 3,\n",
    "            mode=(\"bilinear\", \"bilinear\"),\n",
    "            as_tensor_output=False,\n",
    "        ),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        transforms.RandGaussianNoised(\"image\", prob=0.15, std=0.1),\n",
    "        transforms.RandGaussianSmoothd(\n",
    "            \"image\",\n",
    "            prob=0.15,\n",
    "            sigma_x=(0.5, 1.5),\n",
    "            sigma_y=(0.5, 1.5),\n",
    "            sigma_z=(0.5, 1.5),\n",
    "        ),\n",
    "        transforms.RandScaleIntensityd(\"image\", prob=0.15, factors=0.3),\n",
    "        transforms.RandShiftIntensityd(\"image\", prob=0.15, offsets=0.1),\n",
    "        transforms.RandAdjustContrastd(\"image\", prob=0.15, gamma=(0.7, 1.5)),\n",
    "        transforms.AsDiscreted(\"label\", threshold=0.5),\n",
    "        transforms.ToTensord([\"image\", \"label\"]),\n",
    "    ]\n",
    "    train_transform = transforms.Compose(train_transform)\n",
    "    return train_transform\n",
    "\n",
    "\n",
    "def get_val_transform():\n",
    "    val_transform = [\n",
    "        ft.ReadImaged(\n",
    "            [\"image\", \"label\"], ensure_channel_first=True, allow_missing_keys=True\n",
    "        ),\n",
    "        transforms.SqueezeDimd(\"image\", dim=1),\n",
    "        transforms.NormalizeIntensityd(\"image\", nonzero=True, channel_wise=True),\n",
    "        transforms.ToTensord([\"image\", \"label\"], allow_missing_keys=True),\n",
    "    ]\n",
    "    val_transform = transforms.Compose(val_transform)\n",
    "    return val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff70f31-e0f5-4202-ba5a-f19c3bff644c",
   "metadata": {},
   "source": [
    "## Registry & Read config function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "458a1dce-a704-4607-a2fe-ad501ddfb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torch import nn, optim\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "import factorizer as ft\n",
    "\n",
    "def lambda_constructor(loader, node):\n",
    "    lambda_expr = \"lambda \" + loader.construct_scalar(node)\n",
    "    return eval(lambda_expr)\n",
    "\n",
    "\n",
    "def get_constructor(obj):\n",
    "    \"\"\"Get constructor for an object.\"\"\"\n",
    "\n",
    "    def constructor(loader, node):\n",
    "        if isinstance(node, yaml.nodes.ScalarNode):\n",
    "            if node.value:\n",
    "                out = obj(loader.construct_scalar(node))\n",
    "            else:\n",
    "                out = obj\n",
    "        elif isinstance(node, yaml.nodes.SequenceNode):\n",
    "            out = obj(*loader.construct_sequence(node, deep=True))\n",
    "        elif isinstance(node, yaml.nodes.MappingNode):\n",
    "            out = obj(**loader.construct_mapping(node, deep=True))\n",
    "\n",
    "        return out\n",
    "\n",
    "    return constructor\n",
    "\n",
    "\n",
    "def add_attributes(obj, prefix=\"\"):\n",
    "    for attr_name in dir(obj):\n",
    "        if not attr_name.startswith(\"_\"):\n",
    "            Loader.add_constructor(\n",
    "                f\"!{prefix}{attr_name}\",\n",
    "                get_constructor(getattr(obj, attr_name)),\n",
    "            )\n",
    "\n",
    "\n",
    "Loader = yaml.SafeLoader\n",
    "\n",
    "\n",
    "# general\n",
    "Loader.add_constructor(\"!eval\", get_constructor(eval))\n",
    "Loader.add_constructor(\"!lambda\", lambda_constructor)\n",
    "\n",
    "\n",
    "# pytorch\n",
    "add_attributes(nn, \"nn.\")\n",
    "add_attributes(optim, \"optim.\")\n",
    "\n",
    "\n",
    "# pytorch lightning\n",
    "add_attributes(pl.callbacks, \"pl.\")\n",
    "add_attributes(pl.loggers, \"pl.\")\n",
    "\n",
    "\n",
    "# monai\n",
    "add_attributes(monai.losses, \"monai.\")\n",
    "add_attributes(monai.networks.nets, \"monai.\")\n",
    "\n",
    "\n",
    "# factorizer\n",
    "add_attributes(ft, \"ft.\")\n",
    "\n",
    "\n",
    "def read_config(path, loader=Loader):\n",
    "    with open(path, \"rb\") as file:\n",
    "        config = yaml.load(file, loader)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cda03-8b1d-4faa-9556-885d5ec3b7d6",
   "metadata": {},
   "source": [
    "## Quick checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6754be30-a05b-4a81-89a1-be1df55af35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft.ISLESDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd382267-246e-4493-a2e8-87fa11901f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b44efc11-ebca-48a1-b336-d5a480f8430d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa530b61-529e-4b9e-a8cf-e9c6335a3dbe",
   "metadata": {},
   "source": [
    "## Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1914a40-1b0e-4f05-b5a8-6884d5b8cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISLESDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_properties,\n",
    "        fold=0,\n",
    "        batch_size=2,\n",
    "        num_workers=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_properties = ft.load_properties(data_properties)\n",
    "        self.fold = fold\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.dataset_kwargs = kwargs\n",
    "\n",
    "        self.train_transform = get_train_transform()\n",
    "        self.val_transform = get_val_transform()\n",
    "        self.test_transform = get_val_transform()\n",
    "\n",
    "        self.train_set = self.val_set = self.test_set = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage in (\"fit\", \"validate\", None):\n",
    "            # make training set\n",
    "            training_data = []\n",
    "            val_data = []\n",
    "            for sample in self.data_properties[\"training\"]:\n",
    "                if sample[\"fold\"] == self.fold:\n",
    "                    val_data.append(sample)\n",
    "                else:\n",
    "                    training_data.append(sample)\n",
    "\n",
    "            self.train_set = Dataset(\n",
    "                training_data,\n",
    "                transform=self.train_transform,\n",
    "                **self.dataset_kwargs,\n",
    "            )\n",
    "            self.val_set = Dataset(\n",
    "                val_data,\n",
    "                transform=self.val_transform,\n",
    "                **self.dataset_kwargs,\n",
    "            )\n",
    "\n",
    "        if stage in (\"test\", \"predict\", None):\n",
    "            # make test set\n",
    "            self.test_set = Dataset(\n",
    "                self.data_properties[\"test\"],\n",
    "                transform=self.test_transform,\n",
    "                **self.dataset_kwargs,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val_set,\n",
    "            batch_size=1,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size=1,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "968c32c9-13d7-44ad-a674-d7919c33dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = ISLESDataModule(data_properties = \"/Data/data/isles/dataset_without_flair.json\", fold=0,\n",
    "        batch_size=2,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9cde3c0-2ac9-4935-8f7a-d839b0f24960",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(batch.keys())\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1218\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1216\u001b[39m _utils.signal_handling._set_SIGCHLD_handler()\n\u001b[32m   1217\u001b[39m \u001b[38;5;28mself\u001b[39m._worker_pids_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1260\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._prefetch_factor * \u001b[38;5;28mself\u001b[39m._num_workers):\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1513\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_put_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding < max_tasks\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1513\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:722\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m._sampler_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/sampler.py:347\u001b[39m, in \u001b[36mBatchSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     sampler_iter = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m.sampler)\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drop_last:\n\u001b[32m    349\u001b[39m         \u001b[38;5;66;03m# Create multiple references to the same iterator\u001b[39;00m\n\u001b[32m    350\u001b[39m         args = [sampler_iter] * \u001b[38;5;28mself\u001b[39m.batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/torch/utils/data/sampler.py:123\u001b[39m, in \u001b[36mSequentialSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_source)))\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "batch = next(iter(datamodule.val_dataloader()))\n",
    "# print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e3e34-c14b-4bf0-8afa-a281b40b67ef",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e877b4fa-a16d-46ea-a9df-3437c9cf30e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3.11 /users/eleves-a/2022/oussama.zouhry/factorizer-p ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "attribute 'inferer' removed from hparams because it cannot be pickled\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                   | Params\n",
      "------------------------------------------------\n",
      "0 | net  | SegmentationFactorizer | 7.4 M \n",
      "1 | loss | DeepSuprLoss           | 0     \n",
      "------------------------------------------------\n",
      "7.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.486    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a5ac4ba35c42d8b4231dca9fad9514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m trainer = Trainer(**config[\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    606\u001b[39m model = \u001b[38;5;28mself\u001b[39m._maybe_unwrap_optimized(model)\n\u001b[32m    607\u001b[39m \u001b[38;5;28mself\u001b[39m.strategy._lightning_module = model\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:38\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     41\u001b[39m     trainer._call_teardown_hook()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    643\u001b[39m ckpt_path = ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resume_from_checkpoint\n\u001b[32m    644\u001b[39m \u001b[38;5;28mself\u001b[39m._ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._set_ckpt_path(\n\u001b[32m    645\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    646\u001b[39m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    647\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    648\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    649\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    653\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28mself\u001b[39m._checkpoint_connector.restore_training_state()\n\u001b[32m   1110\u001b[39m \u001b[38;5;28mself\u001b[39m._checkpoint_connector.resume_end()\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m log.detail(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1115\u001b[39m \u001b[38;5;28mself\u001b[39m._teardown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m   1190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_predict()\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[39m, in \u001b[36mTrainer._run_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1201\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_training_routine()\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1274\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[38;5;28mself\u001b[39m._call_callback_hooks(\u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1280\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/loops/loop.py:199\u001b[39m, in \u001b[36mLoop.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[39m, in \u001b[36mEvaluationLoop.advance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_dataloaders > \u001b[32m1\u001b[39m:\n\u001b[32m    151\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdataloader_idx\u001b[39m\u001b[33m\"\u001b[39m] = dataloader_idx\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m dl_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m._outputs.append(dl_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/loops/loop.py:199\u001b[39m, in \u001b[36mLoop.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[39m, in \u001b[36mEvaluationEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_started()\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m output = \u001b[38;5;28mself\u001b[39m._evaluation_step_end(output)\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[39m, in \u001b[36mEvaluationEpochLoop._evaluation_step\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[32m    224\u001b[39m \n\u001b[32m    225\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m \u001b[33;03m    the outputs of the step\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    233\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[39m, in \u001b[36mTrainer._call_strategy_hook\u001b[39m\u001b[34m(self, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m   1491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1494\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m   1497\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.precision_plugin.val_step_context():\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, ValidationStep)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/factorizer/utils/lightning.py:101\u001b[39m, in \u001b[36mSemanticSegmentation.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     y, id_ = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, batch[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[32m    104\u001b[39m     y_hat = \u001b[38;5;28mself\u001b[39m.predict_step(batch, batch_idx)\n",
      "\u001b[31mKeyError\u001b[39m: 'target'"
     ]
    }
   ],
   "source": [
    "# from argparse import ArgumentParser, Namespace\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "# get config\n",
    "# parser = ArgumentParser(description=\"\"\"Train the model.\"\"\", add_help=False)\n",
    "# parser.add_argument(\"--config\", type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "path_config = \"/users/eleves-a/2022/oussama.zouhry/factorizer-project/image-segmentation-factorizer/factorizer/configs/isles2022-dwi&adc/config_isles2022-dwi&adc_fold0_swin-factorizer.yaml\"\n",
    "config = read_config(path_config)\n",
    "\n",
    "# data\n",
    "# dm = config[\"data\"]\n",
    "\n",
    "# init model\n",
    "task_cls, task_params = config[\"task\"]\n",
    "if \"checkpoint_path\" in task_params:\n",
    "    model = task_cls.load_from_checkpoint(strict=False, **task_params)\n",
    "else:\n",
    "    model = task_cls(**task_params)\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(**config[\"training\"])\n",
    "\n",
    "# fit model\n",
    "trainer.fit(model, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
