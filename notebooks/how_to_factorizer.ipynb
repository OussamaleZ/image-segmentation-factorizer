{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4185120c-a86b-4a02-84c7-8da04aa45956",
   "metadata": {},
   "source": [
    "The following notebook is based on: https://github.com/pashtari/factorizer-isles22/blob/master/get_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ad9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c286c2df-d770-40b1-aee7-4a6129e521e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6363045-5309-4cb0-b988-29f080f03a56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/pashtari/factorizer.git@0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6ff387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import factorizer as ft\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import pytorch_lightning as pl\n",
    "from monai import transforms\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5848aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n"
     ]
    }
   ],
   "source": [
    "print(12*12*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b11604c-65bf-4d74-8d61-a59c8ef22e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91900275-988d-43d7-95fb-693e6eb64c87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d069829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin_factorizer = ft.factorizer(\n",
    "#     in_channels=4,\n",
    "#     out_channels=3,\n",
    "#     spatial_size=(8, 8, 8),\n",
    "#     encoder_depth=(1, 1, 1),\n",
    "#     encoder_width=(2, 2, 2),\n",
    "#     strides=(1, 2, 2),\n",
    "#     decoder_depth=(1, 1, 1, 1),\n",
    "#     norm=ft.LayerNorm,\n",
    "#     reshape=(ft.SWMatricize, {'head_dim': 2, 'patch_size': 2}),\n",
    "#     act=nn.ReLU,\n",
    "#     factorize=ft.NMF,\n",
    "#     rank=1,\n",
    "#     num_iters=5,\n",
    "#     init=\"uniform\",\n",
    "#     solver=\"hals\",\n",
    "#     mlp_ratio=2,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "\n",
    "# x = torch.rand((1, 4, 8, 8, 8))\n",
    "\n",
    "# y = swin_factorizer(x)\n",
    "# print(\"Output shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f1644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(swin_factorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5db5-b9b2-4d1a-b9f6-fbce70d759ff",
   "metadata": {},
   "source": [
    "## Check image shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d485fe2-e96d-4705-8f3d-e9ca044b423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"/Data/\"))\n",
    "print(os.path.exists(\"/Data/data/isles/sub-strokecase0001/ses-0001/anat/sub-strokecase0001_ses-0001_flair_registered.nii.gz\"))\n",
    "print(os.path.exists(\"Data/data/isles/sub-strokecase0100/ses-0001/dwi/sub-strokecase0100_ses-0001_dwi.nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9123b900-cd6c-4ef4-b3b1-db74a24dc4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unknown type: itkMatrixF44",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     17\u001b[39m load_image = transforms.LoadImaged(\n\u001b[32m     18\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     19\u001b[39m     ensure_channel_first=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     20\u001b[39m     allow_missing_keys=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# load image data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m data = \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimage shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmask shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/monai/transforms/io/dictionary.py:133\u001b[39m, in \u001b[36mLoadImaged.__call__\u001b[39m\u001b[34m(self, data, reader)\u001b[39m\n\u001b[32m    131\u001b[39m d = \u001b[38;5;28mdict\u001b[39m(data)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key_iterator(d, \u001b[38;5;28mself\u001b[39m.meta_keys, \u001b[38;5;28mself\u001b[39m.meta_key_postfix):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loader.image_only:\n\u001b[32m    135\u001b[39m         d[key] = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/monai/transforms/io/array.py:258\u001b[39m, in \u001b[36mLoadImage.__call__\u001b[39m\u001b[34m(self, filename, reader)\u001b[39m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`meta_data` must be a dict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# make sure all elements in metadata are little endian\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m meta_data = \u001b[43mswitch_endianness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m meta_data[Key.FILENAME_OR_OBJ] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mensure_tuple(filename)[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Path obj should be strings for data loader\u001b[39;00m\n\u001b[32m    261\u001b[39m img = MetaTensor.ensure_torch_and_prune_meta(img_array, meta_data, \u001b[38;5;28mself\u001b[39m.simple_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/monai/transforms/io/array.py:84\u001b[39m, in \u001b[36mswitch_endianness\u001b[39m\u001b[34m(data, new)\u001b[39m\n\u001b[32m     82\u001b[39m     data = [switch_endianness(x, new) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     data = \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_endianness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/monai/transforms/io/array.py:84\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     82\u001b[39m     data = [switch_endianness(x, new) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     data = {k: \u001b[43mswitch_endianness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data.items()}\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/factorizer-project/image-segmentation-factorizer/factorizer-env/lib/python3.11/site-packages/monai/transforms/io/array.py:86\u001b[39m, in \u001b[36mswitch_endianness\u001b[39m\u001b[34m(data, new)\u001b[39m\n\u001b[32m     84\u001b[39m     data = {k: switch_endianness(v, new) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data.items()}\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mRuntimeError\u001b[39m: Unknown type: itkMatrixF44"
     ]
    }
   ],
   "source": [
    "# set data path and\n",
    "dataset_dir = \"/Data/data/isles\"\n",
    "\n",
    "# set patient ID and images path\n",
    "id_ = \"sub-strokecase0001\"\n",
    "dwi_path = f\"{dataset_dir}/{id_}/ses-0001/dwi/{id_}_ses-0001_dwi.nii.gz\"\n",
    "adc_path = f\"{dataset_dir}/{id_}/ses-0001/dwi/{id_}_ses-0001_adc.nii.gz\"\n",
    "\n",
    "msk_path = f\"{dataset_dir}/derivatives/{id_}/ses-0001/{id_}_ses-0001_msk.nii.gz\"\n",
    "\n",
    "# make data dictionary\n",
    "data = {\n",
    "    \"image\": [dwi_path, adc_path],\n",
    "    \"mask\": msk_path,\n",
    "}\n",
    "\n",
    "load_image = transforms.LoadImaged(\n",
    "    [\"image\", \"mask\"],\n",
    "    ensure_channel_first=True,\n",
    "    allow_missing_keys=True,\n",
    ")\n",
    "\n",
    "# load image data\n",
    "data = load_image(data)\n",
    "print(f\"image shape: {data['image'].shape}\")\n",
    "print(f\"mask shape: {data['mask'].shape}\")\n",
    "\n",
    "\n",
    "dwi_image = data[\"image\"][0]\n",
    "adc_image = data[\"image\"][1]\n",
    "msk_image = data[\"mask\"][0]\n",
    "\n",
    "# pick a slice with the largest lesion volume for visualization\n",
    "slc = msk_image.sum((0, 1)).argmax()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200)\n",
    "# visulize DWI image\n",
    "ax[0].imshow(dwi_image[:, :, slc], cmap=\"gray\", origin=\"lower\")\n",
    "ax[0].set_title(\"DWI\")\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "# visulize ADC image\n",
    "ax[1].imshow(adc_image[:, :, slc], cmap=\"gray\", origin=\"lower\")\n",
    "ax[1].set_title(\"ADC\")\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "# visulize mask\n",
    "ax[2].imshow(dwi_image[:, :, slc], \"gray\", origin=\"lower\")\n",
    "masked = np.ma.masked_where(msk_image[:, :, slc] == 0, dwi_image[:, :, slc])\n",
    "ax[2].imshow(masked, ListedColormap([\"red\"]), alpha=0.9, origin=\"lower\")\n",
    "ax[2].set_title(\"Ground Truth\")\n",
    "ax[2].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7daa9-012a-4835-ab01-4315227df894",
   "metadata": {},
   "source": [
    "## Setup transforms for training and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29b349a-bf33-497d-b6b3-cd1b6be5a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    train_transform = [\n",
    "        ft.ReadImaged([\"image\", \"label\"], ensure_channel_first=True),\n",
    "        transforms.SqueezeDimd(\"image\", dim=1),\n",
    "        transforms.CropForegroundd([\"image\", \"label\"], source_key=\"image\"),\n",
    "        transforms.NormalizeIntensityd(\"image\", nonzero=True, channel_wise=True),\n",
    "        transforms.Spacingd(\n",
    "            [\"image\", \"label\"],\n",
    "            pixdim=(2.0, 2.0, 2.0),\n",
    "            mode=(\"bilinear\", \"bilinear\"),\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            [\"image\", \"label\"], roi_size=(64, 64, 64), random_size=False\n",
    "        ),\n",
    "        transforms.RandAffined(\n",
    "            [\"image\", \"label\"],\n",
    "            prob=0.15,\n",
    "            spatial_size=(64, 64, 64),\n",
    "            rotate_range=[30 * np.pi / 180] * 3,\n",
    "            scale_range=[0.3] * 3,\n",
    "            mode=(\"bilinear\", \"bilinear\"),\n",
    "            as_tensor_output=False,\n",
    "        ),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        transforms.RandFlipd([\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        transforms.RandGaussianNoised(\"image\", prob=0.15, std=0.1),\n",
    "        transforms.RandGaussianSmoothd(\n",
    "            \"image\",\n",
    "            prob=0.15,\n",
    "            sigma_x=(0.5, 1.5),\n",
    "            sigma_y=(0.5, 1.5),\n",
    "            sigma_z=(0.5, 1.5),\n",
    "        ),\n",
    "        transforms.RandScaleIntensityd(\"image\", prob=0.15, factors=0.3),\n",
    "        transforms.RandShiftIntensityd(\"image\", prob=0.15, offsets=0.1),\n",
    "        transforms.RandAdjustContrastd(\"image\", prob=0.15, gamma=(0.7, 1.5)),\n",
    "        transforms.AsDiscreted(\"label\", threshold=0.5),\n",
    "        transforms.ToTensord([\"image\", \"label\"]),\n",
    "    ]\n",
    "    train_transform = transforms.Compose(train_transform)\n",
    "    return train_transform\n",
    "\n",
    "\n",
    "def get_val_transform():\n",
    "    val_transform = [\n",
    "        ft.ReadImaged(\n",
    "            [\"image\", \"label\"], ensure_channel_first=True, allow_missing_keys=True\n",
    "        ),\n",
    "        transforms.SqueezeDimd(\"image\", dim=1),\n",
    "        transforms.NormalizeIntensityd(\"image\", nonzero=True, channel_wise=True),\n",
    "        transforms.ToTensord([\"image\", \"label\"], allow_missing_keys=True),\n",
    "    ]\n",
    "    val_transform = transforms.Compose(val_transform)\n",
    "    return val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff70f31-e0f5-4202-ba5a-f19c3bff644c",
   "metadata": {},
   "source": [
    "## Registry & Read config function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458a1dce-a704-4607-a2fe-ad501ddfb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torch import nn, optim\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "import factorizer as ft\n",
    "\n",
    "def lambda_constructor(loader, node):\n",
    "    lambda_expr = \"lambda \" + loader.construct_scalar(node)\n",
    "    return eval(lambda_expr)\n",
    "\n",
    "\n",
    "def get_constructor(obj):\n",
    "    \"\"\"Get constructor for an object.\"\"\"\n",
    "\n",
    "    def constructor(loader, node):\n",
    "        if isinstance(node, yaml.nodes.ScalarNode):\n",
    "            if node.value:\n",
    "                out = obj(loader.construct_scalar(node))\n",
    "            else:\n",
    "                out = obj\n",
    "        elif isinstance(node, yaml.nodes.SequenceNode):\n",
    "            out = obj(*loader.construct_sequence(node, deep=True))\n",
    "        elif isinstance(node, yaml.nodes.MappingNode):\n",
    "            out = obj(**loader.construct_mapping(node, deep=True))\n",
    "\n",
    "        return out\n",
    "\n",
    "    return constructor\n",
    "\n",
    "\n",
    "def add_attributes(obj, prefix=\"\"):\n",
    "    for attr_name in dir(obj):\n",
    "        if not attr_name.startswith(\"_\"):\n",
    "            Loader.add_constructor(\n",
    "                f\"!{prefix}{attr_name}\",\n",
    "                get_constructor(getattr(obj, attr_name)),\n",
    "            )\n",
    "\n",
    "\n",
    "Loader = yaml.SafeLoader\n",
    "\n",
    "\n",
    "# general\n",
    "Loader.add_constructor(\"!eval\", get_constructor(eval))\n",
    "Loader.add_constructor(\"!lambda\", lambda_constructor)\n",
    "\n",
    "\n",
    "# pytorch\n",
    "add_attributes(nn, \"nn.\")\n",
    "add_attributes(optim, \"optim.\")\n",
    "\n",
    "\n",
    "# pytorch lightning\n",
    "add_attributes(pl.callbacks, \"pl.\")\n",
    "add_attributes(pl.loggers, \"pl.\")\n",
    "\n",
    "\n",
    "# monai\n",
    "add_attributes(monai.losses, \"monai.\")\n",
    "add_attributes(monai.networks.nets, \"monai.\")\n",
    "\n",
    "\n",
    "# factorizer\n",
    "add_attributes(ft, \"ft.\")\n",
    "\n",
    "\n",
    "def read_config(path, loader=Loader):\n",
    "    with open(path, \"rb\") as file:\n",
    "        config = yaml.load(file, loader)\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cda03-8b1d-4faa-9556-885d5ec3b7d6",
   "metadata": {},
   "source": [
    "## Quick checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6754be30-a05b-4a81-89a1-be1df55af35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft.ISLESDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd382267-246e-4493-a2e8-87fa11901f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b44efc11-ebca-48a1-b336-d5a480f8430d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa530b61-529e-4b9e-a8cf-e9c6335a3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e3e34-c14b-4bf0-8afa-a281b40b67ef",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e877b4fa-a16d-46ea-a9df-3437c9cf30e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     model = task_cls(**task_params)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# init trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m trainer = \u001b[43mTrainer\u001b[49m(**config[\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[32m     24\u001b[39m trainer.fit(model, dm)\n",
      "\u001b[31mNameError\u001b[39m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# from argparse import ArgumentParser, Namespace\n",
    "\n",
    "# get config\n",
    "# parser = ArgumentParser(description=\"\"\"Train the model.\"\"\", add_help=False)\n",
    "# parser.add_argument(\"--config\", type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "path_config = \"/users/eleves-a/2022/oussama.zouhry/factorizer-project/image-segmentation-factorizer/factorizer/configs/isles2022-dwi&adc/config_isles2022-dwi&adc_fold0_swin-factorizer.yaml\"\n",
    "config = read_config(path_config)\n",
    "\n",
    "# data\n",
    "dm = config[\"data\"]\n",
    "\n",
    "# init model\n",
    "task_cls, task_params = config[\"task\"]\n",
    "if \"checkpoint_path\" in task_params:\n",
    "    model = task_cls.load_from_checkpoint(strict=False, **task_params)\n",
    "else:\n",
    "    model = task_cls(**task_params)\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(**config[\"training\"])\n",
    "\n",
    "# fit model\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6fb81-b79a-4a5c-9a26-8b7f2633e85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
